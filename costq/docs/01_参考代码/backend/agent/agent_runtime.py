"""
AgentCore Runtime å…¥å£æ–‡ä»¶

å°† CostQ Agent éƒ¨ç½²åˆ° AWS AgentCore Runtimeã€‚
ä½¿ç”¨ bedrock-agentcore SDK çš„æ ‡å‡†æ¨¡å¼ï¼Œç›´æ¥è¿”å›ç»“æœã€‚

æ¶æ„:
    Client (boto3)
      â†’ AgentCore Runtime (æœ¬æ–‡ä»¶)
        â†’ Strands Agent
          â†’ MCP Servers (STDio å­è¿›ç¨‹)

å…³é”®è®¾è®¡:
- âœ… ç¬¦åˆ AgentCore å®˜æ–¹æ ‡å‡†ï¼ˆreturn è€Œä¸æ˜¯ yieldï¼‰
- âœ… ç›´æ¥è°ƒç”¨ Strands Agentï¼ˆä¸ä½¿ç”¨ StreamingAgentWrapperï¼‰
- âœ… MCP æœåŠ¡å™¨ä½œä¸ºå­è¿›ç¨‹å¯åŠ¨ï¼ˆSTDio é€šä¿¡ï¼‰
- âœ… ç®€å•ç›´æ¥ï¼ˆ~100 è¡Œæ ¸å¿ƒä»£ç ï¼‰
"""

# ========== æ ‡å‡†åº“å¯¼å…¥ ==========
import logging
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any

# ========== ç¬¬ä¸‰æ–¹åº“å¯¼å…¥ ==========
from bedrock_agentcore import BedrockAgentCoreApp
from opentelemetry import baggage, context, trace

# ========== æœ¬åœ°æ¨¡å—å¯¼å…¥ ==========
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from backend.agent.agent_manager import AgentManager
from backend.mcp.mcp_manager import MCPManager

# ========== å…¨å±€å˜é‡åˆå§‹åŒ– ==========
logger = logging.getLogger(__name__)
tracer = trace.get_tracer(__name__)

RUNTIME_START_FILE = "/tmp/.runtime_start_time"


# ========== Runtime å¯åŠ¨æ—¶é—´è¿½è¸ª ==========
def get_runtime_start_time():
    """è·å–æˆ–åˆ›å»ºRuntimeå¯åŠ¨æ—¶é—´ï¼ˆæŒä¹…åŒ–åˆ°æ–‡ä»¶ï¼Œå®¹å™¨çº§åˆ«ï¼‰"""
    if os.path.exists(RUNTIME_START_FILE):
        try:
            with open(RUNTIME_START_FILE) as f:
                return float(f.read())
        except Exception:
            pass
    start_time = time.time()
    try:
        with open(RUNTIME_START_FILE, "w") as f:
            f.write(str(start_time))
    except Exception:
        pass
    return start_time


RUNTIME_START_TIME = get_runtime_start_time()


def get_runtime_uptime():
    """è·å–Runtimeè¿è¡Œæ—¶é•¿ï¼ˆç§’ï¼‰"""
    return time.time() - RUNTIME_START_TIME


def is_cold_start(threshold_seconds=60):
    """åˆ¤æ–­æ˜¯å¦å†·å¯åŠ¨ï¼ˆå¯åŠ¨å60ç§’å†…è®¤ä¸ºæ˜¯å†·å¯åŠ¨ï¼‰"""
    return get_runtime_uptime() < threshold_seconds


# ========== Memory å®¢æˆ·ç«¯ï¼ˆå…¨å±€å•ä¾‹ï¼‰==========
_memory_client = None
_memory_id = None


def _get_or_create_memory_client():
    """è·å–æˆ–åˆ›å»º MemoryClientï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰

    Returns:
        Tuple[Optional[MemoryClient], Optional[str]]: (memory_client, memory_id)
            - memory_client: AgentCore Memory Client å®ä¾‹
            - memory_id: Memory Resource ID
    """
    global _memory_client, _memory_id
    if _memory_client is not None:
        return (_memory_client, _memory_id)
    try:
        from bedrock_agentcore.memory import MemoryClient

        from backend.config.settings import settings

        memory_region = settings.AWS_REGION
        _memory_client = MemoryClient(region_name=memory_region)
        _memory_id = settings.MEMORY_RESOURCE_ID
        if not _memory_id:
            logger.warning("âš ï¸ MEMORY_RESOURCE_ID æœªé…ç½®ï¼ŒMemory åŠŸèƒ½å°†è¢«ç¦ç”¨")
            _memory_client = None
            return (None, None)
        logger.info(
            f"âœ… AgentCore Memory Client åˆå§‹åŒ–æˆåŠŸ - Region: {memory_region}, Memory ID: {_memory_id}"
        )
        return (_memory_client, _memory_id)
    except ImportError as e:
        logger.warning(f"âš ï¸ bedrock-agentcore SDK æœªå®‰è£…: {e}")
        _memory_client = None
        _memory_id = None
        return (None, None)
    except Exception as e:
        logger.error(f"âŒ Memory Client åˆå§‹åŒ–å¤±è´¥: {e}")
        _memory_client = None
        _memory_id = None
        return (None, None)


_get_or_create_memory_client()
app = BedrockAgentCoreApp(debug=True)
mcp_manager = None
agent_manager = None


def get_or_create_managers():
    """è·å–æˆ–åˆ›å»ºå…¨å±€ç®¡ç†å™¨

    åªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶åˆ›å»ºï¼Œåç»­å¤ç”¨ã€‚
    è¿™æ ·å¯ä»¥é¿å…æ¯æ¬¡è¯·æ±‚éƒ½é‡æ–°åˆ›å»º BedrockModel å’Œ MCP è¿æ¥ã€‚

    Returns:
        Tuple: (mcp_manager, agent_manager, dialog_system_prompt, alert_system_prompt)
    """
    global mcp_manager, agent_manager
    if mcp_manager is None:
        logger.info("åˆ›å»º MCPManager...")
        mcp_manager = MCPManager()
    if agent_manager is None:
        logger.info("åˆ›å»º AgentManager...")
        from backend.agent.prompts import get_aws_intelligent_agent_prompt
        from backend.agent.prompts.alert_agent import ALERT_EXECUTION_SYSTEM_PROMPT
        from backend.config.settings import settings

        dialog_system_prompt = get_aws_intelligent_agent_prompt(
            platform="AWS", include_examples=True
        )
        logger.info(f"âœ… å¯¹è¯æç¤ºè¯åŠ è½½å®Œæˆ - é•¿åº¦: {len(dialog_system_prompt)} å­—ç¬¦")
        alert_system_prompt = ALERT_EXECUTION_SYSTEM_PROMPT
        logger.info(f"âœ… å‘Šè­¦æç¤ºè¯åŠ è½½å®Œæˆ - é•¿åº¦: {len(alert_system_prompt)} å­—ç¬¦")
        agent_manager = AgentManager(
            system_prompt=dialog_system_prompt, model_id=settings.BEDROCK_MODEL_ID
        )
        logger.info("âœ… é»˜è®¤ AgentManager å·²åˆ›å»ºï¼ˆå¯¹è¯åœºæ™¯ï¼‰")
    from backend.agent.prompts import get_aws_intelligent_agent_prompt
    from backend.agent.prompts.alert_agent import ALERT_EXECUTION_SYSTEM_PROMPT

    dialog_system_prompt = get_aws_intelligent_agent_prompt(platform="AWS", include_examples=True)
    alert_system_prompt = ALERT_EXECUTION_SYSTEM_PROMPT
    return (mcp_manager, agent_manager, dialog_system_prompt, alert_system_prompt)


def log_tool_call(tool_name: str, tool_id: str, tool_input: dict):
    """è®°å½•å·¥å…·è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯

    è¿™äº›æ—¥å¿—ä¼šè¢« OpenTelemetry é‡‡é›†å¹¶å‘é€åˆ° CloudWatch
    ä½¿ç”¨ logger.info() çš„ extra å‚æ•°ä¼ é€’ç»“æ„åŒ–æ•°æ®
    """
    import json

    logger.info(
        f"ğŸ”§ TOOL CALL START - {tool_name}",
        extra={
            "tool_name": tool_name,
            "tool_id": tool_id,
            "tool_input": json.dumps(tool_input, ensure_ascii=False),
            "event_type": "tool_call_start",
        },
    )


def log_tool_result(tool_id: str, tool_result: dict, status: str = "success"):
    """è®°å½•å·¥å…·æ‰§è¡Œç»“æœ

    ä½¿ç”¨ logger.info() çš„ extra å‚æ•°ä¼ é€’ç»“æ„åŒ–æ•°æ®
    """
    import json

    result_preview = json.dumps(tool_result, ensure_ascii=False)[:500]
    logger.info(
        f"âœ… TOOL RESULT - {status}",
        extra={
            "tool_id": tool_id,
            "tool_result": result_preview,
            "status": status,
            "event_type": "tool_result",
        },
    )


def filter_event(event: dict) -> dict:
    """
    è¿‡æ»¤SSEäº‹ä»¶å†—ä½™å­—æ®µï¼Œé¿å…è§¦å‘100MB Runtimeé™åˆ¶

    ç­–ç•¥ï¼šé»‘åå•è¿‡æ»¤ - åªç§»é™¤å·²çŸ¥çš„å†—ä½™å¤§å­—æ®µï¼Œä¿ç•™æ‰€æœ‰å…¶ä»–å­—æ®µ

    æ ¹å› ï¼š
    - AWS Bedrock AgentCore Runtime é™åˆ¶ï¼šMaximum payload size = 100 MB
    - Strands Agent æ¯ä¸ªtokenäº‹ä»¶æºå¸¦å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆ~100KBï¼‰
    - å¯¼è‡´çº¦1049ä¸ªäº‹ä»¶åè§¦å‘é™åˆ¶

    è§£å†³ï¼š
    - ç§»é™¤å†—ä½™å­—æ®µï¼ˆagentã€request_stateç­‰ï¼Œ~99KBï¼‰
    - ä¿ç•™å¿…éœ€å­—æ®µï¼ˆeventã€deltaã€dataç­‰ï¼‰
    - äº‹ä»¶å¤§å°ä» ~100KB å‡å°‘åˆ° ~500å­—èŠ‚ï¼ˆ99.5%å‡å°‘ï¼‰
    - æ”¯æŒäº‹ä»¶æ•°ä» 1,049 å¢åŠ åˆ° ~200,000ï¼ˆ190å€æå‡ï¼‰

    ä¼˜åŠ¿ï¼š
    - å®‰å…¨ï¼šä¸ä¼šæ„å¤–è¿‡æ»¤æ‰æœªçŸ¥ä½†é‡è¦çš„å°å­—æ®µ
    - ç®€å•ï¼šåªéœ€ç»´æŠ¤"ç§»é™¤åˆ—è¡¨"
    - é²æ£’ï¼šå…¼å®¹æœªæ¥å¯èƒ½æ–°å¢çš„å­—æ®µ

    Args:
        event: åŸå§‹Strandsäº‹ä»¶

    Returns:
        è¿‡æ»¤åçš„äº‹ä»¶ï¼ˆç§»é™¤äº†å†—ä½™å¤§å­—æ®µï¼‰
    """
    remove_fields = {
        "agent",
        "request_state",
        "event_loop_cycle_trace",
        "event_loop_cycle_span",
        "model",
        "messages",
        "system_prompt",
        "tool_config",
        "event_loop_cycle_id",
    }
    filtered = {k: v for k, v in event.items() if k not in remove_fields}
    if not hasattr(filter_event, "_logged_stats"):
        import json

        try:
            original_size = len(json.dumps(event, ensure_ascii=False, default=str))
            filtered_size = len(json.dumps(filtered, ensure_ascii=False, default=str))
            reduction_ratio = (1 - filtered_size / original_size) * 100 if original_size > 0 else 0
            logger.info(
                f"âœ… Event filtering initialized - original: {original_size}B, filtered: {filtered_size}B, reduction: {round(reduction_ratio, 2)}%, optimization: {round(original_size / filtered_size, 1)}x"
            )
            filter_event._logged_stats = True
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to log filter stats: {e}")
    return filtered


@app.entrypoint
async def invoke(payload: dict[str, Any]):
    """
    AgentCore Runtime å…¥å£å‡½æ•°ï¼ˆæµå¼è¾“å‡ºç‰ˆæœ¬ï¼‰

    ä½¿ç”¨å®˜æ–¹æ¨èçš„ async + stream_async + yield æ¨¡å¼ï¼š
    1. æ¥æ”¶ payload
    2. ä»æ•°æ®åº“æŸ¥è¯¢è´¦å·ä¿¡æ¯
    3. Runtime å†…éƒ¨æ‰§è¡Œä¸¤ä¸ª AssumeRole
    4. æ ¹æ® prompt_type é€‰æ‹©å¯¹åº”çš„æç¤ºè¯
    5. åˆ›å»º Agentï¼ˆå‘Šè­¦åœºæ™¯ä¸ä½¿ç”¨ Memoryï¼‰
    6. æµå¼æ‰§è¡Œ Agentï¼ˆyield æ¯ä¸ªäº‹ä»¶ï¼‰

    Args:
        payload: è°ƒç”¨å‚æ•°
            - prompt: ç”¨æˆ·æŸ¥è¯¢ï¼ˆå¿…éœ€ï¼‰
            - account_id: AWS è´¦å· IDï¼ˆå¿…éœ€ï¼‰
            - account_type: è´¦å·ç±»å‹ï¼ˆé»˜è®¤: awsï¼Œå¯é€‰: gcpï¼‰
            - prompt_type: æç¤ºè¯ç±»å‹ï¼ˆé»˜è®¤: "dialog"ï¼‰
                * "dialog": ä½¿ç”¨å¯¹è¯æç¤ºè¯ï¼Œå¯ç”¨ Memory
                * "alert": ä½¿ç”¨å‘Šè­¦æç¤ºè¯ï¼Œç¦ç”¨ Memory
            - session_id: ä¼šè¯ IDï¼ˆå¯é€‰ï¼Œå¯¹è¯åœºæ™¯ä½¿ç”¨ï¼‰
            - user_id: ç”¨æˆ· IDï¼ˆå¯é€‰ï¼Œå¯¹è¯åœºæ™¯ä½¿ç”¨ï¼‰
            - org_id: ç»„ç»‡ IDï¼ˆå¯é€‰ï¼Œå¯¹è¯åœºæ™¯ä½¿ç”¨ï¼‰

    Yields:
        Dict[str, Any]: æµå¼äº‹ä»¶
            - å·¥å…·è°ƒç”¨äº‹ä»¶
            - æ–‡æœ¬ç”Ÿæˆäº‹ä»¶
            - æœ€ç»ˆç»“æœäº‹ä»¶

    Examples:
        >>> # å¯¹è¯åœºæ™¯ï¼ˆé»˜è®¤ï¼‰
        >>> payload = {
        ...     "prompt": "æŸ¥è¯¢æˆæœ¬",
        ...     "account_id": "123456789012",
        ...     "session_id": "sess-123",
        ...     "user_id": "user-456",
        ... }
        >>> # prompt_type é»˜è®¤ä¸º "dialog"ï¼Œå¯çœç•¥

        >>> # å‘Šè­¦åœºæ™¯
        >>> payload = {
        ...     "prompt": "å½“æ—¥ EC2 æˆæœ¬è¶…è¿‡ $1000",
        ...     "account_id": "123456789012",
        ...     "prompt_type": "alert",  # âœ… å…³é”®å‚æ•°
        ... }
        >>> # å‘Šè­¦åœºæ™¯ä¸éœ€è¦ session_idã€user_id
    """
    import json

    invoke_start_time = time.time()
    runtime_uptime = get_runtime_uptime()
    is_cold = is_cold_start(threshold_seconds=60)
    logger.info(
        "ğŸš€ AgentCore Runtime invocation started ...",
        extra={
            "payload_keys": list(payload.keys()),
            "runtime_uptime_seconds": round(runtime_uptime, 2),
            "is_cold_start": is_cold,
        },
    )
    with tracer.start_as_current_span("agent.invocation") as root_span:
        rds_secret_name = os.getenv("RDS_SECRET_NAME")
        if not rds_secret_name:
            error_msg = "Missing required environment variable: RDS_SECRET_NAME"
            logger.error(error_msg)
            root_span.set_status(trace.Status(trace.StatusCode.ERROR, error_msg))
            yield {"error": error_msg}
            return
        logger.info(
            "Database secret loaded from environment", extra={"secret_name": rds_secret_name}
        )
        encryption_key = os.getenv("ENCRYPTION_KEY")
        if not encryption_key:
            error_msg = "Missing required environment variable: ENCRYPTION_KEY"
            logger.error(error_msg)
            root_span.set_status(trace.Status(trace.StatusCode.ERROR, error_msg))
            yield {"error": error_msg}
            return
        logger.info("Encryption key loaded from environment")
        step1_start = time.time()
        user_message = payload.get("prompt")
        if not user_message:
            error_msg = "Missing required parameter: prompt"
            logger.error(error_msg, extra={"payload": payload})
            root_span.set_status(trace.Status(trace.StatusCode.ERROR, error_msg))
            yield {"error": error_msg}
            return
        account_id = payload.get("account_id")
        if not account_id:
            error_msg = "Missing required parameter: account_id"
            logger.error(error_msg, extra={"payload": payload})
            root_span.set_status(trace.Status(trace.StatusCode.ERROR, error_msg))
            yield {"error": error_msg}
            return
        prompt_type = payload.get("prompt_type", "dialog")
        if prompt_type not in ["dialog", "alert"]:
            error_msg = f"Invalid prompt_type: {prompt_type}. Must be 'dialog' or 'alert'"
            logger.error(error_msg, extra={"payload": payload})
            root_span.set_status(trace.Status(trace.StatusCode.ERROR, error_msg))
            yield {"error": error_msg}
            return
        logger.info("Prompt type determined", extra={"prompt_type": prompt_type})
        root_span.set_attribute("prompt.type", prompt_type)
        account_type = payload.get("account_type", "aws")
        session_id = payload.get("session_id")
        user_id = payload.get("user_id")
        org_id = payload.get("org_id")
        step1_duration = time.time() - step1_start
        logger.debug(
            "â±ï¸ Step 1: Payloadè§£æå®Œæˆ",
            extra={
                "duration_seconds": round(step1_duration, 3),
                "prompt_type": prompt_type,
                "has_session_id": session_id is not None,
            },
        )
        original_session_id = session_id
        session_renewed = False
        if session_id and prompt_type == "dialog":
            session_check_start = time.time()
            SESSION_MAX_AGE = 7 * 3600
            db = None
            try:
                from sqlalchemy import text

                from backend.database import get_db

                db = next(get_db())
                sql = text(
                    "\n                    SELECT\n                        EXTRACT(EPOCH FROM (NOW() - created_at)) as age_seconds\n                    FROM chat_sessions\n                    WHERE id = :session_id\n                "
                )
                result = db.execute(sql, {"session_id": session_id}).fetchone()
                if result:
                    session_age = result[0]
                    if session_age > SESSION_MAX_AGE:
                        import uuid

                        new_session_id = str(uuid.uuid4())
                        logger.warning(
                            "Sessionæ¥è¿‘è¿‡æœŸï¼Œåˆ›å»ºæ–°session",
                            extra={
                                "old_session_id": str(session_id),
                                "new_session_id": new_session_id,
                                "session_age_hours": session_age / 3600,
                                "max_age_hours": SESSION_MAX_AGE / 3600,
                            },
                        )
                        session_id = new_session_id
                        session_renewed = True
                else:
                    import uuid

                    new_session_id = str(uuid.uuid4())
                    logger.warning(
                        "Sessionä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°session",
                        extra={"old_session_id": str(session_id), "new_session_id": new_session_id},
                    )
                    session_id = new_session_id
                    session_renewed = True
            except Exception as e:
                session_check_duration = time.time() - session_check_start
                logger.warning(
                    "â±ï¸ Sessionè¿‡æœŸæ£€æµ‹å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸsession",
                    extra={
                        "error": str(e),
                        "session_id": str(session_id),
                        "duration_seconds": round(session_check_duration, 3),
                    },
                )
            finally:
                if "session_check_start" in locals():
                    session_check_duration = time.time() - session_check_start
                    logger.debug(
                        "â±ï¸ Sessionæ£€æµ‹å®Œæˆ",
                        extra={
                            "duration_seconds": round(session_check_duration, 3),
                            "session_renewed": session_renewed,
                        },
                    )
                if db is not None:
                    db.close()
                    logger.debug("æ•°æ®åº“è¿æ¥å·²å…³é—­")
        root_span.set_attribute("session.id", session_id or "")
        root_span.set_attribute("session.renewed", session_renewed)
        root_span.set_attribute("user.id", user_id or "")
        root_span.set_attribute("account.id", account_id)
        root_span.set_attribute("account.type", account_type)
        root_span.set_attribute("prompt.length", len(user_message))
        logger.info(
            "Parameters validated",
            extra={
                "prompt_length": len(user_message),
                "account_id": account_id,
                "account_type": account_type,
                "has_session_id": session_id is not None,
                "has_user_id": user_id is not None,
                "session_renewed": session_renewed,
            },
        )
        if session_renewed:
            yield {
                "type": "session_renewed",
                "old_session_id": original_session_id,
                "new_session_id": session_id,
                "reason": "session_expired",
                "message": "ä¼šè¯å·²è¿‡æœŸï¼Œå·²è‡ªåŠ¨åˆ›å»ºæ–°ä¼šè¯",
            }
        context_token = None
        if session_id:
            ctx = baggage.set_baggage("session.id", session_id)
            if user_id:
                ctx = baggage.set_baggage("user.id", user_id, context=ctx)
            context_token = context.attach(ctx)
            logger.info(
                "ğŸ”— Session context set",
                extra={
                    "session_id": str(session_id),
                    "user_id": str(user_id),
                    "event_type": "session_context",
                },
            )
        db_query_start = time.time()
        logger.info("â±ï¸ Step 2: æ•°æ®åº“æŸ¥è¯¢å¼€å§‹", extra={"account_id": account_id})
        db = None
        with tracer.start_as_current_span("database.query_account") as db_span:
            db_span.set_attribute("db.operation", "SELECT")
            db_span.set_attribute("account.type", account_type)
            db_span.set_attribute("account.id", account_id)
            try:
                from backend.database import get_db

                logger.info("Database modules imported successfully")
                db = next(get_db())
                logger.info("Database session created")
                from sqlalchemy import text

                if account_type == "gcp":
                    db_span.set_attribute("db.table", "gcp_accounts")
                    sql = text(
                        """
                        SELECT id, project_id, account_name, credentials_encrypted, org_id
                        FROM gcp_accounts
                        WHERE id = :account_id
                        ORDER BY created_at DESC
                        LIMIT 1
                    """
                    )
                    logger.debug(
                        "ğŸ“Š Using GCP query",
                        extra={"table": "gcp_accounts", "account_id": account_id},
                    )
                else:
                    db_span.set_attribute("db.table", "aws_accounts")
                    sql = text(
                        """
                        SELECT id, account_id, role_arn, org_id, region, auth_type,
                               access_key_id, secret_access_key_encrypted
                        FROM aws_accounts
                        WHERE id = :account_id
                        ORDER BY created_at DESC
                        LIMIT 1
                    """
                    )
                    logger.debug(
                        "ğŸ“Š Using AWS query",
                        extra={"table": "aws_accounts", "account_id": account_id},
                    )
                logger.info(
                    "Executing database query",
                    extra={"account_id": account_id, "account_type": account_type},
                )
                sql_exec_start = time.time()
                result = db.execute(sql, {"account_id": account_id}).fetchone()
                sql_exec_duration = time.time() - sql_exec_start
                logger.debug(
                    "â±ï¸ SQLæ‰§è¡Œå®Œæˆ",
                    extra={
                        "duration_seconds": round(sql_exec_duration, 3),
                        "result_found": bool(result),
                    },
                )
                if not result:
                    error_msg = f"Account not found: {account_id} (type: {account_type})"
                    logger.error(
                        error_msg,
                        extra={
                            "account_id": account_id,
                            "account_type": account_type,
                            "sql_query": str(sql),
                            "table": "gcp_accounts" if account_type == "gcp" else "aws_accounts",
                        },
                    )
                    db_span.set_status(trace.Status(trace.StatusCode.ERROR, error_msg))
                    db_span.set_attribute("account.found", False)
                    root_span.set_status(trace.Status(trace.StatusCode.ERROR, error_msg))
                    yield {"error": error_msg}
                    return
                db_span.set_attribute("account.found", True)
                if account_type == "gcp":
                    account_uuid = result[0]
                    project_id = result[1]
                    account_name = result[2]
                    credentials_encrypted = result[3]
                    org_id = result[4]
                    account_id_db = project_id
                    role_arn = None
                    region = None
                    auth_type = "service_account"
                    access_key_id = None
                    secret_key_encrypted = credentials_encrypted
                    db_span.set_attribute("gcp.project_id", project_id)
                    db_span.set_attribute("gcp.account_name", account_name)
                else:
                    account_uuid = result[0]
                    account_id_db = result[1]
                    role_arn = result[2]
                    org_id = result[3]
                    region = result[4] or "us-east-1"
                    auth_type = result[5] or "aksk"
                    access_key_id = result[6]
                    secret_key_encrypted = result[7]
                    db_span.set_attribute("auth.type", auth_type)
                    db_span.set_attribute("account.region", region)
                db_query_duration = time.time() - db_query_start
                logger.info(
                    "â±ï¸ Account info retrieved successfully",
                    extra={
                        "account_uuid": str(account_uuid),
                        "account_id": account_id_db,
                        "auth_type": auth_type,
                        "org_id": str(org_id),
                        "region": region,
                        "db_query_duration_seconds": round(db_query_duration, 3),
                    },
                )
            except ValueError as e:
                error_msg = f"Invalid account_id parameter: {str(e)}"
                logger.error(
                    error_msg, extra={"account_id": account_id, "error_type": "ValidationError"}
                )
                db_span.set_status(trace.Status(trace.StatusCode.ERROR, error_msg))
                root_span.set_status(trace.Status(trace.StatusCode.ERROR, error_msg))
                yield {"error": error_msg, "error_type": "client_error"}
                return
            except ImportError as e:
                error_msg = f"Database module import failed: {str(e)}"
                logger.error(error_msg, extra={"error_type": "ConfigurationError"})
                import traceback

                logger.error("Import error traceback", extra={"traceback": traceback.format_exc()})
                db_span.set_status(trace.Status(trace.StatusCode.ERROR, error_msg))
                root_span.set_status(trace.Status(trace.StatusCode.ERROR, error_msg))
                yield {"error": error_msg, "error_type": "server_error"}
                return
            except Exception as e:
                error_msg = f"Database query failed: {str(e)}"
                logger.error(
                    error_msg,
                    extra={
                        "account_id": account_id,
                        "error_type": type(e).__name__,
                        "error_details": str(e),
                    },
                )
                import traceback

                logger.error(
                    "Database query traceback", extra={"traceback": traceback.format_exc()}
                )
                db_span.set_status(trace.Status(trace.StatusCode.ERROR, error_msg))
                root_span.set_status(trace.Status(trace.StatusCode.ERROR, error_msg))
                yield {"error": error_msg, "error_type": "database_error"}
                return
            finally:
                if db is not None:
                    db.close()
                    logger.info("Database session closed")
    logger.info("Step 3: Creating managers (before setting env vars)")
    try:
        mcp_mgr, agent_mgr, dialog_system_prompt, alert_system_prompt = get_or_create_managers()
        logger.info(
            "Managers created successfully",
            extra={
                "has_mcp_manager": mcp_mgr is not None,
                "has_agent_manager": agent_mgr is not None,
                "dialog_prompt_len": len(dialog_system_prompt),
                "alert_prompt_len": len(alert_system_prompt),
            },
        )
    except ImportError as e:
        error_msg = f"Failed to import manager modules: {str(e)}"
        logger.error(error_msg, extra={"error_type": "ImportError", "error_details": str(e)})
        import traceback

        logger.error("Manager import traceback", extra={"traceback": traceback.format_exc()})
        yield {"error": error_msg, "error_type": "server_error"}
        return
    except ValueError as e:
        error_msg = f"Invalid manager configuration: {str(e)}"
        logger.error(error_msg, extra={"error_type": "ConfigurationError"})
        yield {"error": error_msg, "error_type": "configuration_error"}
        return
    except Exception as e:
        error_msg = f"Failed to create managers: {str(e)}"
        logger.error(error_msg, extra={"error_type": type(e).__name__, "error_details": str(e)})
        import traceback

        logger.error("Manager creation traceback", extra={"traceback": traceback.format_exc()})
        yield {"error": error_msg, "error_type": "internal_error"}
        return
    # ========== âœ… ã€å…³é”®ä¿®æ”¹ã€‘åˆ›å»ºéš”ç¦»çš„ç¯å¢ƒå˜é‡å­—å…¸ ==========
    # ä¸å†ä½¿ç”¨ os.environ è®¾ç½®å‡­è¯ï¼Œæ”¹ä¸ºä½¿ç”¨ additional_env å­—å…¸
    # è¿™æ ·å¯ä»¥é¿å…æ±¡æŸ“ä¸»è¿›ç¨‹ç¯å¢ƒï¼ŒOpenTelemetry ä¼šç»§ç»­ä½¿ç”¨ Runtime IAM Role
    additional_env: dict[str, str] = {}

    # âœ… ç”¨äºæ¸…ç† GCP ä¸´æ—¶å‡­è¯æ–‡ä»¶
    gcp_temp_file: str | None = None

    credentials_start = time.time()
    logger.info(
        f"Step 4: Getting credentials (auth_type={auth_type}, using env isolation)",
        extra={
            "auth_type": auth_type,
            "org_id": str(org_id),
            "region": region,
            "env_isolation_enabled": True  # âœ… æ ‡è®°ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡éš”ç¦»
        },
    )
    if auth_type == "iam_role":
        if not role_arn:
            error_msg = f"IAM Role not configured for account: {account_id}"
            logger.error(error_msg, extra={"account_id": account_id})
            yield {"error": error_msg}
            return
        try:
            from backend.services.iam_role_session_factory import IAMRoleSessionFactory

            logger.info("IAMRoleSessionFactory imported")
            external_id = f"org-{org_id}" if org_id else None
            logger.info("Generated external_id", extra={"external_id": external_id})
            logger.info("Creating IAMRoleSessionFactory instance")
            target_factory = IAMRoleSessionFactory.get_instance(
                account_id=account_id, role_arn=role_arn, external_id=external_id, region=region
            )
            logger.info("IAMRoleSessionFactory instance created")
            logger.info("Getting temporary credentials")
            target_credentials = target_factory.get_current_credentials()
            logger.info(
                "IAM Role credentials obtained (storing to isolated env dict)",
                extra={
                    "access_key_prefix": target_credentials["access_key_id"][:20],
                    "has_secret_key": bool(target_credentials.get("secret_access_key")),
                    "has_session_token": bool(target_credentials.get("session_token")),
                    "env_isolation": True,  # âœ… æ ‡è®°ï¼šéš”ç¦»å­˜å‚¨
                },
            )
            # âœ… å­˜å‚¨åˆ°éš”ç¦»å­—å…¸ï¼Œä¸è®¾ç½® os.environï¼ˆé¿å…æ±¡æŸ“ä¸»è¿›ç¨‹ï¼‰
            additional_env["AWS_ACCESS_KEY_ID"] = target_credentials["access_key_id"]
            additional_env["AWS_SECRET_ACCESS_KEY"] = target_credentials["secret_access_key"]
            additional_env["AWS_SESSION_TOKEN"] = target_credentials["session_token"]
        except Exception as e:
            error_msg = f"AssumeRole to target account failed: {str(e)}"
            logger.error(
                error_msg,
                extra={
                    "role_arn": role_arn,
                    "account_id": account_id,
                    "error_type": type(e).__name__,
                    "error_details": str(e),
                },
            )
            import traceback

            logger.error("AssumeRole traceback", extra={"traceback": traceback.format_exc()})
            yield {"error": error_msg}
            return
    elif auth_type == "service_account":
        if not secret_key_encrypted:
            error_msg = f"Service account JSON not configured for GCP account: {account_id}"
            logger.error(error_msg, extra={"account_id": account_id})
            yield {"error": error_msg}
            return
        try:
            import json
            import tempfile

            from backend.services.gcp_credential_manager import get_gcp_credential_manager

            logger.info("Decrypting GCP service account JSON")
            gcp_credential_manager = get_gcp_credential_manager()
            service_account_json = gcp_credential_manager.decrypt_credentials(secret_key_encrypted)
            logger.info(
                "GCP service account JSON decrypted successfully",
                extra={"project_id": account_id_db},
            )
            with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
                json.dump(service_account_json, f)
                service_account_file = f.name
                gcp_temp_file = service_account_file  # âœ… è®°å½•æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºåç»­æ¸…ç†
            # âœ… å­˜å‚¨åˆ°éš”ç¦»å­—å…¸ï¼Œä¸è®¾ç½® os.environ
            additional_env["GOOGLE_APPLICATION_CREDENTIALS"] = service_account_file
            additional_env["GCP_PROJECT_ID"] = account_id_db
            additional_env["GCP_ACCOUNT_ID"] = account_id
            logger.info(
                "GCP credentials set successfully (storing to isolated env dict)",
                extra={
                    "project_id": account_id_db,
                    "credentials_file": service_account_file,
                    "env_isolation": True,  # âœ… æ ‡è®°ï¼šéš”ç¦»å­˜å‚¨
                },
            )
        except Exception as e:
            error_msg = f"Failed to decrypt GCP service account JSON: {str(e)}"
            logger.error(
                error_msg,
                extra={
                    "account_id": account_id,
                    "error_type": type(e).__name__,
                    "error_details": str(e),
                },
            )
            import traceback

            logger.error("GCP credentials traceback", extra={"traceback": traceback.format_exc()})
            yield {"error": error_msg}
            return
    else:
        if not access_key_id or not secret_key_encrypted:
            error_msg = f"AKSK credentials not configured for account: {account_id}"
            logger.error(error_msg, extra={"account_id": account_id})
            yield {"error": error_msg}
            return
        try:
            from backend.services.credential_manager import get_credential_manager

            logger.info("Decrypting AKSK credentials")
            credential_manager = get_credential_manager()
            secret_access_key = credential_manager.decrypt_secret_key(secret_key_encrypted)
            logger.info(
                "AKSK credentials decrypted successfully (storing to isolated env dict)",
                extra={
                    "access_key_prefix": access_key_id[:20],
                    "env_isolation": True,  # âœ… æ ‡è®°ï¼šéš”ç¦»å­˜å‚¨
                },
            )
            # âœ… å­˜å‚¨åˆ°éš”ç¦»å­—å…¸ï¼Œä¸è®¾ç½® os.environ
            additional_env["AWS_ACCESS_KEY_ID"] = access_key_id
            additional_env["AWS_SECRET_ACCESS_KEY"] = secret_access_key
            # âœ… ç¡®ä¿ä¸ä¼ é€’ SESSION_TOKENï¼ˆAKSK ä¸éœ€è¦ï¼‰
            # æ³¨æ„ï¼šä¸éœ€è¦åˆ é™¤ os.environ ä¸­çš„ï¼Œå› ä¸ºæˆ‘ä»¬æ ¹æœ¬æ²¡è®¾ç½®
        except Exception as e:
            error_msg = f"Failed to decrypt AKSK credentials: {str(e)}"
            logger.error(
                error_msg,
                extra={
                    "account_id": account_id,
                    "error_type": type(e).__name__,
                    "error_details": str(e),
                },
            )
            import traceback

            logger.error("AKSK decryption traceback", extra={"traceback": traceback.format_exc()})
            yield {"error": error_msg}
            return
    if account_type == "gcp":
        logger.info(
            "âœ… GCP å‡­è¯å·²å‡†å¤‡ï¼ˆéš”ç¦»å­—å…¸ï¼Œä¸æ±¡æŸ“ä¸»è¿›ç¨‹ï¼‰",
            extra={"env_isolation": True}
        )
    else:
        # âœ… AWS åŒºåŸŸä¿¡æ¯ä¹Ÿå­˜å‚¨åˆ°éš”ç¦»å­—å…¸
        additional_env["AWS_REGION"] = region
        additional_env["AWS_DEFAULT_REGION"] = region
        is_container = os.environ.get("DOCKER_CONTAINER") == "1"
        if not is_container:
            # å¹³å° Profile ä¼ é€’ç»™ MCPï¼ˆæœ¬åœ°å¼€å‘ä½¿ç”¨ï¼‰
            additional_env["PLATFORM_AWS_PROFILE"] = os.environ.get("AWS_PROFILE", "3532")
            logger.info(f"è®¾ç½®å¹³å° Profileï¼ˆéš”ç¦»ä¼ é€’ï¼‰: {additional_env['PLATFORM_AWS_PROFILE']}")
        logger.info(
            f"âœ… {auth_type.upper()} å‡­è¯å·²å‡†å¤‡ï¼ˆéš”ç¦»å­—å…¸ï¼Œä¸æ±¡æŸ“ä¸»è¿›ç¨‹ï¼‰",
            extra={
                "auth_type": auth_type,
                "env_isolation": True,
                "env_vars_count": len(additional_env)
            }
        )
    credentials_duration = time.time() - credentials_start
    logger.info(
        "â±ï¸ Step 3-5: AWSå‡­è¯è·å–å®Œæˆ",
        extra={"auth_type": auth_type, "duration_seconds": round(credentials_duration, 3)},
    )
    clients_dict = None
    with tracer.start_as_current_span("mcp.initialize") as mcp_span:
        try:
            mcp_start_time = time.time()
            logger.info("åˆ›å»º MCP å®¢æˆ·ç«¯...")
            from backend.config.settings import settings

            if account_type == "gcp":
                available_mcps = settings.GCP_MCP_SERVERS
                logger.info("GCPåœºæ™¯ï¼šåªæœ‰1ä¸ªMCPï¼Œå¹¶è¡Œä¼˜åŒ–æ•ˆæœæœ‰é™")
            else:
                available_mcps = settings.AWS_MCP_SERVERS
                logger.info(f"AWSåœºæ™¯ï¼š{len(available_mcps)}ä¸ªMCP")
            mcp_span.set_attribute("mcp.account_type", account_type)
            mcp_span.set_attribute("mcp.servers_requested", len(available_mcps))
            logger.info(
                "Step 6: Creating MCP clients (with env isolation)",
                extra={
                    "available_mcps": available_mcps,
                    "mcp_count": len(available_mcps),
                    "env_isolation_enabled": True,  # âœ… æ ‡è®°ï¼šç¯å¢ƒå˜é‡éš”ç¦»
                    "additional_env_count": len(additional_env)
                },
            )
            # âœ… ä¼ é€’éš”ç¦»çš„ç¯å¢ƒå˜é‡ç»™ MCP Clientsï¼ˆä¸æ±¡æŸ“ä¸»è¿›ç¨‹ï¼‰
            clients_dict = mcp_mgr.create_all_clients(
                server_types=available_mcps,
                additional_env=additional_env  # âœ… å…³é”®ï¼šéš”ç¦»ä¼ é€’
            )
            mcp_elapsed = time.time() - mcp_start_time
            mcp_span.set_attribute("mcp.clients_created", len(clients_dict))
            mcp_span.set_attribute("mcp.elapsed_seconds", round(mcp_elapsed, 2))

            # âœ… éªŒè¯ä¸»è¿›ç¨‹ç¯å¢ƒå˜é‡æ²¡æœ‰è¢«æ±¡æŸ“ï¼ˆä½¿ç”¨ä¸“ç”¨éªŒè¯å‡½æ•°ï¼‰
            from backend.utils.env_isolation_validator import verify_env_isolation

            isolation_ok = verify_env_isolation(phase="after_mcp_creation")
            if not isolation_ok:
                logger.error(
                    "ğŸš¨ ä¸¥é‡ï¼šç¯å¢ƒå˜é‡éš”ç¦»å¤±è´¥ï¼",
                    extra={
                        "phase": "after_mcp_creation",
                        "impact": "OpenTelemetry/Bedrock/Memory å¯èƒ½ä½¿ç”¨äº†é”™è¯¯çš„å‡­è¯"
                    }
                )

            logger.info(
                "MCP clients created (env isolation verified)",
                extra={
                    "success_count": len(clients_dict),
                    "requested_count": len(available_mcps),
                    "created_types": list(clients_dict.keys()),
                    "elapsed_seconds": round(mcp_elapsed, 2),
                    "env_isolation_verified": isolation_ok,  # âœ… ä½¿ç”¨éªŒè¯å‡½æ•°çš„ç»“æœ
                },
            )
            tools = []
            tool_details = {}

            # ========== 1. æ”¶é›†æœ¬åœ° MCP å·¥å…·ï¼ˆstdio æ¨¡å¼ï¼‰==========
            for server_type, client in clients_dict.items():
                try:
                    logger.info(f"Getting tools from {server_type}")
                    server_tools = client.list_tools_sync()
                    tools.extend(server_tools)
                    tool_details[server_type] = len(server_tools)
                    logger.info(
                        f"âœ… Tools from {server_type}", extra={"tool_count": len(server_tools)}
                    )
                except Exception as e:
                    logger.error(
                        f"âŒ Failed to load tools from {server_type}",
                        extra={
                            "server_type": server_type,
                            "error_type": type(e).__name__,
                            "error": str(e),
                        },
                    )
                    tool_details[server_type] = 0

            local_tools_count = len(tools)
            logger.info(
                "Local MCP tools loaded",
                extra={"local_tools_count": local_tools_count, "tools_per_mcp": tool_details}
            )

            # ========== 2. æ”¶é›† Gateway MCP å·¥å…·ï¼ˆHTTP + SigV4 æ¨¡å¼ï¼‰==========
            # ä»…å½“é…ç½®äº† Gateway URL ä¸”æ˜¯ AWS è´¦å·æ—¶åŠ è½½
            gateway_url = settings.COSTQ_AWS_MCP_SERVERS_GATEWAY_URL
            if gateway_url and account_type == "aws":
                try:
                    logger.info(
                        "Step 6.1: Creating Gateway MCP client (SigV4)",
                        extra={
                            "gateway_url": gateway_url[:50] + "..." if len(gateway_url) > 50 else gateway_url,
                            "gateway_mcps": settings.AWS_GATEWAY_MCP_SERVERS,
                        }
                    )
                    gateway_client = mcp_mgr.create_gateway_client(name="gateway-mcp")
                    gateway_client.__enter__()  # æ¿€æ´»è¿æ¥

                    # è·å–å®Œæ•´å·¥å…·åˆ—è¡¨ï¼ˆå¤„ç†åˆ†é¡µï¼‰
                    gateway_tools = mcp_mgr.get_full_tools_list(gateway_client)
                    tools.extend(gateway_tools)
                    tool_details["gateway"] = len(gateway_tools)

                    logger.info(
                        "âœ… Gateway MCP tools loaded",
                        extra={
                            "gateway_tools_count": len(gateway_tools),
                            "gateway_mcps": settings.AWS_GATEWAY_MCP_SERVERS,
                        }
                    )

                    # æ³¨æ„ï¼šgateway_client éœ€è¦åœ¨ Agent ä½¿ç”¨å®Œæ¯•åå…³é—­
                    # è¿™é‡Œå…ˆä¿å­˜å¼•ç”¨ï¼Œåç»­åœ¨æ¸…ç†é˜¶æ®µå¤„ç†
                    clients_dict["gateway"] = gateway_client

                except Exception as e:
                    logger.error(
                        "âŒ Failed to load Gateway MCP tools",
                        extra={
                            "error_type": type(e).__name__,
                            "error": str(e),
                            "gateway_url": gateway_url[:50] + "..." if len(gateway_url) > 50 else gateway_url,
                        },
                    )
                    tool_details["gateway"] = 0
            else:
                if not gateway_url:
                    logger.info("Gateway MCP æœªé…ç½®ï¼ˆCOSTQ_AWS_MCP_SERVERS_GATEWAY_URL æœªè®¾ç½®ï¼‰ï¼Œè·³è¿‡")
                elif account_type != "aws":
                    logger.info(f"Gateway MCP ä»…æ”¯æŒ AWS è´¦å·ï¼Œå½“å‰è´¦å·ç±»å‹: {account_type}")

            mcp_span.set_attribute("mcp.total_tools", len(tools))
            mcp_span.set_attribute("mcp.local_tools", local_tools_count)
            mcp_span.set_attribute("mcp.gateway_tools", len(tools) - local_tools_count)
            logger.info(
                "All tools loaded (local + gateway)",
                extra={
                    "total_tools": len(tools),
                    "local_tools": local_tools_count,
                    "gateway_tools": len(tools) - local_tools_count,
                    "tools_per_mcp": tool_details
                }
            )
        except Exception as e:
            logger.error(f"åˆ›å»º MCP å®¢æˆ·ç«¯å¤±è´¥: {e}")
            import traceback

            logger.error(traceback.format_exc())
            mcp_span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
            root_span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
            yield {"error": f"Failed to create MCP clients: {str(e)}"}
            return
    # âœ… ä¸å†éœ€è¦æ¸…ç†ç¯å¢ƒå˜é‡ï¼ˆå› ä¸ºä»æœªæ±¡æŸ“ os.environï¼‰
    # æŸ¥è¯¢è´¦å·å‡­è¯ä»…åœ¨ additional_env å­—å…¸ä¸­ï¼Œå·²éš MCP å­è¿›ç¨‹ä¼ é€’
    # ä¸»è¿›ç¨‹ç¯å¢ƒå˜é‡ä¿æŒå¹²å‡€ï¼ŒOpenTelemetry/Bedrock/Memory ç»§ç»­ä½¿ç”¨ Runtime IAM Role
    logger.info(
        "âœ… ç¯å¢ƒå˜é‡éš”ç¦»æˆåŠŸï¼šä¸»è¿›ç¨‹æœªè¢«æ±¡æŸ“",
        extra={
            "auth_type": auth_type,
            "env_isolation_verified": "AWS_ACCESS_KEY_ID" not in os.environ,
            "benefit": "OpenTelemetry/Bedrock/Memory ç»§ç»­ä½¿ç”¨ Runtime IAM Roleï¼ˆä¸å—æŸ¥è¯¢è´¦å·å½±å“ï¼‰"
        }
    )
    step7_start_time = time.time()
    try:
        logger.info(
            "â±ï¸ SPAN START: Step 7 - Agentåˆ›å»º",
            extra={
                "tool_count": len(tools),
                "prompt_type": prompt_type,
                "has_session_id": session_id is not None,
                "has_org_id": org_id is not None,
            },
        )
        memory_client = None
        memory_id = None
        if prompt_type == "alert":
            logger.info("åˆ›å»ºå‘Šè­¦ Agentï¼ˆä½¿ç”¨å‘Šè­¦æç¤ºè¯ï¼Œæ—  Memoryï¼‰")
            alert_agent_manager = AgentManager(
                system_prompt=alert_system_prompt, model_id=settings.BEDROCK_MODEL_ID
            )
            agent = alert_agent_manager.create_agent_with_memory(tools=tools)
            logger.info(
                "Agent åˆ›å»ºå®Œæˆï¼ˆå‘Šè­¦åœºæ™¯ï¼‰", extra={"has_memory": False, "tool_count": len(tools)}
            )
        else:
            logger.info("åˆ›å»ºå¯¹è¯ Agentï¼ˆå°è¯•Memoryæ¨¡å¼ï¼‰")
            agent_created = False
            memory_fallback_reason = None
            executor = None
            try:
                memory_init_start = time.time()
                memory_client, memory_id = _get_or_create_memory_client()
                memory_init_duration = time.time() - memory_init_start
                logger.info(
                    "â±ï¸ Memoryå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ",
                    extra={
                        "memory_id": memory_id,
                        "duration_seconds": round(memory_init_duration, 2),
                    },
                )
                import asyncio
                from concurrent.futures import ThreadPoolExecutor

                executor = ThreadPoolExecutor(max_workers=1)
                agent_create_start = time.time()

                async def create_with_timeout():
                    loop = asyncio.get_event_loop()
                    return await loop.run_in_executor(
                        executor,
                        agent_mgr.create_agent_with_memory,
                        tools,
                        memory_client,
                        memory_id,
                        user_id,
                        session_id,
                        40,
                    )

                agent = await asyncio.wait_for(create_with_timeout(), timeout=30.0)
                agent_create_duration = time.time() - agent_create_start
                if agent is None:
                    raise ValueError("Agentåˆ›å»ºè¿”å›None")
                if not hasattr(agent, "stream_async"):
                    raise ValueError("Agentå¯¹è±¡ç¼ºå°‘stream_asyncæ–¹æ³•")
                agent_created = True
                logger.info(
                    "â±ï¸ Agentåˆ›å»ºå®Œæˆï¼ˆå¯¹è¯åœºæ™¯ï¼ŒMemoryæ¨¡å¼ï¼‰",
                    extra={
                        "has_memory": True,
                        "session_id": str(session_id),
                        "user_id": str(user_id),
                        "tool_count": len(tools),
                        "duration_seconds": round(agent_create_duration, 2),
                    },
                )
            except TimeoutError:
                memory_fallback_reason = "Memoryåˆå§‹åŒ–è¶…æ—¶ï¼ˆ30ç§’ï¼‰"
                logger.warning(
                    memory_fallback_reason,
                    extra={"session_id": str(session_id), "user_id": str(user_id)},
                )
            except Exception as memory_error:
                memory_fallback_reason = f"Memoryåˆå§‹åŒ–å¤±è´¥: {str(memory_error)}"
                logger.warning(
                    "Memoryåˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ°æ— Memoryæ¨¡å¼",
                    extra={
                        "error_type": type(memory_error).__name__,
                        "error_message": str(memory_error),
                        "session_id": str(session_id),
                    },
                )
            finally:
                if executor is not None:
                    executor.shutdown(wait=False)
                    logger.debug("ThreadPoolExecutorå·²å…³é—­")
            if not agent_created:
                try:
                    logger.info("ä½¿ç”¨æ— Memoryæ¨¡å¼åˆ›å»ºAgentï¼ˆå›é€€ï¼‰")
                    agent = agent_mgr.create_agent_with_memory(tools=tools)
                    if agent is None:
                        raise ValueError("Agentåˆ›å»ºè¿”å›Noneï¼ˆæ— Memoryæ¨¡å¼ï¼‰")
                    if not hasattr(agent, "stream_async"):
                        raise ValueError("Agentå¯¹è±¡ç¼ºå°‘stream_asyncæ–¹æ³•ï¼ˆæ— Memoryæ¨¡å¼ï¼‰")
                    logger.info(
                        "Agent åˆ›å»ºå®Œæˆï¼ˆå¯¹è¯åœºæ™¯ï¼Œæ— Memoryæ¨¡å¼ - å›é€€ï¼‰",
                        extra={
                            "has_memory": False,
                            "tool_count": len(tools),
                            "fallback_reason": memory_fallback_reason,
                        },
                    )
                except Exception as fallback_error:
                    error_msg = f"Agentåˆ›å»ºå®Œå…¨å¤±è´¥ï¼ˆåŒ…æ‹¬å›é€€ï¼‰: {str(fallback_error)}"
                    logger.error(
                        error_msg,
                        extra={
                            "original_error": memory_fallback_reason,
                            "fallback_error": str(fallback_error),
                        },
                    )
                    raise ValueError(error_msg)
        step7_duration = time.time() - step7_start_time
        logger.info(
            "â±ï¸ SPAN END: Step 7 - Agentåˆ›å»ºå®Œæˆ",
            extra={"total_duration_seconds": round(step7_duration, 2)},
        )
    except Exception as e:
        step7_duration = time.time() - step7_start_time
        error_msg = f"Failed to create agent: {str(e)}"
        logger.error(
            "â±ï¸ SPAN END: Step 7 - Agentåˆ›å»ºå¤±è´¥",
            extra={
                "error_msg": error_msg,
                "error_type": type(e).__name__,
                "error_details": str(e),
                "duration_seconds": round(step7_duration, 2),
            },
        )
        import traceback

        logger.error("Agent creation traceback")
        root_span.set_status(trace.Status(trace.StatusCode.ERROR, error_msg))
        yield {"error": error_msg}
        return
    stream_start_time = time.time()
    event_count = 0
    last_event_time = stream_start_time

    # Token ä½¿ç”¨ç»Ÿè®¡ï¼ˆæµå¼ç»“æŸåå‘é€ç»™å‰ç«¯ï¼‰
    token_usage: dict[str, int | float] = {
        "input_tokens": 0,
        "output_tokens": 0,
        "cache_read_tokens": 0,
        "cache_write_tokens": 0,
        "input_cache_hit_rate": 0.0,
        "output_cache_hit_rate": 0.0,
    }

    with tracer.start_as_current_span("agent.execute") as exec_span:
        try:
            exec_span.set_attribute("agent.prompt", user_message[:200])
            exec_span.set_attribute(
                "agent.has_memory", memory_client is not None and session_id is not None
            )
            logger.info(
                "â±ï¸ SPAN START: Step 8 - Agentæµå¼æ‰§è¡Œ",
                extra={
                    "prompt": (
                        user_message[:100] + "..." if len(user_message) > 100 else user_message
                    )
                },
            )
            stream = agent.stream_async(user_message)
            logger.info("Agent stream started")
            async for event in stream:
                event_count += 1
                current_time = time.time()
                event_interval = current_time - last_event_time
                if event_interval > 5.0:
                    event_type = "unknown"
                    if isinstance(event, dict) and "event" in event:
                        event_data = event["event"]
                        if "contentBlockStart" in event_data:
                            event_type = "tool_start"
                        elif "contentBlockDelta" in event_data:
                            event_type = "text_delta"
                    logger.warning(
                        "â±ï¸ é•¿é—´éš”äº‹ä»¶æ£€æµ‹",
                        extra={
                            "interval_seconds": round(event_interval, 2),
                            "event_type": event_type,
                            "event_count": event_count,
                            "cumulative_seconds": round(current_time - stream_start_time, 2),
                        },
                    )
                last_event_time = current_time
                if isinstance(event, dict):
                    if "event" in event:
                        event_data = event["event"]
                        if "contentBlockStart" in event_data:
                            start = event_data["contentBlockStart"].get("start", {})
                            if "toolUse" in start:
                                tool_use = start["toolUse"]
                                tool_name = tool_use.get("name")
                                tool_id = tool_use.get("toolUseId")
                                tool_span = tracer.start_span(
                                    f"tool.{tool_name}",
                                    attributes={
                                        "tool.name": tool_name,
                                        "tool.id": tool_id,
                                        "tool.input": json.dumps(tool_use.get("input", {}))[:500],
                                    },
                                )
                                if not hasattr(exec_span, "_tool_spans"):
                                    exec_span._tool_spans = {}
                                exec_span._tool_spans[tool_id] = tool_span
                                log_tool_call(
                                    tool_name=tool_name,
                                    tool_id=tool_id,
                                    tool_input=tool_use.get("input", {}),
                                )
                    if "message" in event:
                        message = event["message"]
                        if message.get("role") == "user":
                            for content in message.get("content", []):
                                if isinstance(content, dict) and "toolResult" in content:
                                    tool_result = content["toolResult"]
                                    tool_id = tool_result.get("toolUseId")
                                    result_data = {}
                                    for item in tool_result.get("content", []):
                                        if isinstance(item, dict):
                                            if "json" in item:
                                                try:
                                                    result_data = (
                                                        item["json"]
                                                        if isinstance(item["json"], dict)
                                                        else json.loads(item["json"])
                                                    )
                                                except:
                                                    result_data = {"raw": str(item["json"])}
                                            elif "text" in item:
                                                result_data = {"text": item["text"]}
                                    if (
                                        hasattr(exec_span, "_tool_spans")
                                        and tool_id in exec_span._tool_spans
                                    ):
                                        tool_span = exec_span._tool_spans[tool_id]
                                        tool_span.set_attribute(
                                            "tool.status", tool_result.get("status", "success")
                                        )
                                        tool_span.set_attribute(
                                            "tool.result", json.dumps(result_data)[:500]
                                        )
                                        tool_span.end()
                                        del exec_span._tool_spans[tool_id]
                                    log_tool_result(
                                        tool_id=tool_id,
                                        tool_result=result_data,
                                        status=tool_result.get("status", "success"),
                                    )

                    # æå– Token ä½¿ç”¨ç»Ÿè®¡ï¼ˆä» result äº‹ä»¶çš„ metrics.accumulated_usage è·å–ï¼‰
                    if "result" in event:
                        try:
                            result = event["result"]
                            if hasattr(result, "metrics") and result.metrics:
                                metrics = result.metrics
                                if hasattr(metrics, "accumulated_usage"):
                                    usage_data = metrics.accumulated_usage
                                    # Strands SDK ä½¿ç”¨é©¼å³°å‘½å
                                    input_tokens = max(0, usage_data.get("inputTokens", 0))
                                    output_tokens = max(0, usage_data.get("outputTokens", 0))
                                    cache_read_tokens = max(
                                        0, usage_data.get("cacheReadInputTokens", 0)
                                    )
                                    cache_write_tokens = max(
                                        0, usage_data.get("cacheWriteInputTokens", 0)
                                    )

                                    # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
                                    total_input = input_tokens + cache_read_tokens
                                    input_cache_hit_rate = (
                                        (cache_read_tokens / total_input * 100)
                                        if total_input > 0
                                        else 0.0
                                    )
                                    # è¾“å‡ºç¼“å­˜ï¼ˆBedrock æš‚ä¸æ”¯æŒï¼Œé¢„ç•™ï¼‰
                                    cache_read_output = usage_data.get("cacheReadOutputTokens", 0)
                                    total_output = output_tokens + cache_read_output
                                    output_cache_hit_rate = (
                                        (cache_read_output / total_output * 100)
                                        if total_output > 0
                                        else 0.0
                                    )

                                    token_usage.update({
                                        "input_tokens": input_tokens,
                                        "output_tokens": output_tokens,
                                        "cache_read_tokens": cache_read_tokens,
                                        "cache_write_tokens": cache_write_tokens,
                                        "input_cache_hit_rate": round(input_cache_hit_rate, 1),
                                        "output_cache_hit_rate": round(output_cache_hit_rate, 1),
                                    })

                                    logger.info(
                                        "Token ç»Ÿè®¡å·²æå–",
                                        extra={
                                            "input": input_tokens,
                                            "output": output_tokens,
                                            "cache_read": cache_read_tokens,
                                            "cache_write": cache_write_tokens,
                                            "input_cache_hit_rate": f"{input_cache_hit_rate:.1f}%",
                                        },
                                    )
                                else:
                                    logger.warning("Result.metrics æ²¡æœ‰ accumulated_usage å±æ€§")
                            else:
                                logger.debug("Result æ²¡æœ‰ metrics å±æ€§æˆ– metrics ä¸ºç©º")
                        except Exception as e:
                            logger.warning(
                                "Token ç»Ÿè®¡æå–å¤±è´¥",
                                extra={"error": str(e), "error_type": type(e).__name__},
                            )

                    logger.debug(
                        "Yielding Bedrock event (before filter)",
                        extra={"event_keys": list(event.keys())},
                    )
                    filtered_event = filter_event(event)
                    logger.debug(
                        "Yielding filtered event",
                        extra={
                            "filtered_keys": list(filtered_event.keys()),
                            "original_count": len(event.keys()),
                            "filtered_count": len(filtered_event.keys()),
                        },
                    )
                    yield filtered_event
                else:
                    logger.debug(
                        "Skipping non-dict event", extra={"event_type": type(event).__name__}
                    )

            # æµå¼ç»“æŸåå‘é€ Token ä½¿ç”¨ç»Ÿè®¡
            # âœ… ä¿®å¤ï¼šæ£€æŸ¥æ‰€æœ‰ token ç±»å‹ï¼Œé¿å…ç¼“å­˜å‘½ä¸­ç‡100%æ—¶ä¸å‘é€
            total_tokens = (
                token_usage["input_tokens"]
                + token_usage["output_tokens"]
                + token_usage["cache_read_tokens"]
                + token_usage["cache_write_tokens"]
            )

            if total_tokens > 0:
                yield {
                    "type": "token_usage",
                    "usage": token_usage,
                    "timestamp": time.time(),
                }
                logger.info(
                    "Token ç»Ÿè®¡äº‹ä»¶å·²å‘é€",
                    extra={
                        "input": token_usage["input_tokens"],
                        "output": token_usage["output_tokens"],
                        "cache_read": token_usage["cache_read_tokens"],
                        "cache_write": token_usage["cache_write_tokens"],
                        "input_cache_hit_rate": f"{token_usage['input_cache_hit_rate']}%",
                        "total_tokens": total_tokens,
                    },
                )

            exec_span.set_status(trace.Status(trace.StatusCode.OK))
            stream_duration = time.time() - stream_start_time
            avg_interval = stream_duration / event_count if event_count > 0 else 0
            logger.info(
                "â±ï¸ SPAN END: Step 8 - Agentæµå¼æ‰§è¡Œå®Œæˆ",
                extra={
                    "total_duration_seconds": round(stream_duration, 2),
                    "event_count": event_count,
                    "avg_interval_seconds": round(avg_interval, 3),
                },
            )
        except Exception as e:
            stream_duration = time.time() - stream_start_time
            error_msg = f"Agent execution failed: {str(e)}"
            logger.error(
                "â±ï¸ SPAN END: Step 8 - Agentæµå¼æ‰§è¡Œå¤±è´¥",
                extra={
                    "error_msg": error_msg,
                    "error_type": type(e).__name__,
                    "error_details": str(e),
                    "duration_seconds": round(stream_duration, 2),
                    "event_count": event_count,
                },
            )
            import traceback

            logger.error("Agent execution traceback", extra={"traceback": traceback.format_exc()})
            exec_span.set_status(trace.Status(trace.StatusCode.ERROR, error_msg))
            root_span.set_status(trace.Status(trace.StatusCode.ERROR, error_msg))
            yield {"error": str(e), "type": type(e).__name__}
        finally:
            # âœ… æ¸…ç† GCP ä¸´æ—¶å‡­è¯æ–‡ä»¶ï¼ˆé˜²æ­¢æ•æ„Ÿä¿¡æ¯æ³„æ¼ï¼‰
            if gcp_temp_file:
                try:
                    # ä½¿ç”¨å…¨å±€çš„ os æ¨¡å—ï¼ˆå·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥ï¼‰
                    if os.path.exists(gcp_temp_file):
                        os.unlink(gcp_temp_file)
                        logger.info(
                            "âœ… GCP ä¸´æ—¶å‡­è¯æ–‡ä»¶å·²æ¸…ç†",
                            extra={"file": gcp_temp_file}
                        )
                except Exception as e:
                    logger.warning(
                        "âš ï¸ æ¸…ç† GCP ä¸´æ—¶æ–‡ä»¶å¤±è´¥",
                        extra={
                            "file": gcp_temp_file,
                            "error": str(e),
                            "error_type": type(e).__name__
                        }
                    )

            if context_token is not None:
                try:
                    context.detach(context_token)
                    logger.debug("OpenTelemetry context detached")
                except Exception as e:
                    logger.error(
                        "Failed to detach OpenTelemetry context",
                        extra={"error": str(e), "error_type": type(e).__name__},
                    )
            if "clients_dict" in locals() and clients_dict:
                logger.info("Cleaning up MCP clients", extra={"client_count": len(clients_dict)})
                for server_type, client in clients_dict.items():
                    try:
                        client.__exit__(None, None, None)
                        logger.debug("MCP client cleaned", extra={"server_type": server_type})
                    except Exception as e:
                        logger.error(
                            "Failed to clean MCP client",
                            extra={
                                "server_type": server_type,
                                "error": str(e),
                                "error_type": type(e).__name__,
                            },
                        )
            invoke_duration = time.time() - invoke_start_time
            breakdown = {}
            if "step7_start_time" in locals():
                step7_dur = locals().get("step7_duration", 0)
                breakdown["agent_creation"] = round(step7_dur, 2)
            if "stream_start_time" in locals():
                stream_dur = time.time() - stream_start_time
                breakdown["streaming"] = round(stream_dur, 2)
            explained_time = sum(breakdown.values())
            unexplained_time = invoke_duration - explained_time
            logger.info(
                "â±ï¸ INVOKE END - å…¨å±€è®¡æ—¶æ€»ç»“",
                extra={
                    "total_duration_seconds": round(invoke_duration, 2),
                    "breakdown_seconds": breakdown,
                    "unexplained_seconds": round(unexplained_time, 2),
                    "unexplained_percentage": (
                        round(unexplained_time / invoke_duration * 100, 1)
                        if invoke_duration > 0
                        else 0
                    ),
                },
            )


if __name__ == "__main__":
    '\n    æœ¬åœ°æµ‹è¯•æ¨¡å¼\n\n    è¿è¡Œæ–¹å¼:\n    python agent_runtime.py\n\n    æµ‹è¯•å‘½ä»¤:\n    curl -X POST http://localhost:8080/invocations       -H "Content-Type: application/json"       -d \'{"prompt": "Hello!"}\'\n'
    import argparse
    import json
    from datetime import datetime

    parser = argparse.ArgumentParser(description="CostQ AgentCore Runtime")
    parser.add_argument("--host", default="0.0.0.0", help="Host to bind")
    parser.add_argument("--port", default=8080, type=int, help="Port to bind")
    parser.add_argument("--log-level", default="INFO", help="Log level")
    args = parser.parse_args()
    logging.basicConfig(
        level=args.log_level.upper(),
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[logging.StreamHandler(sys.stderr)],
    )
    startup_log = {
        "timestamp": datetime.now().isoformat(),
        "level": "INFO",
        "message": "ğŸš€ CostQ AgentCore Runtime starting",
        "host": args.host,
        "port": args.port,
        "log_level": args.log_level,
        "model_id": os.getenv("BEDROCK_MODEL_ID"),
        "python_version": sys.version,
        "environment": {
            "AWS_REGION": os.getenv("AWS_REGION"),
            "AWS_DEFAULT_REGION": os.getenv("AWS_DEFAULT_REGION"),
            "BEDROCK_REGION": os.getenv("BEDROCK_REGION"),
            "RDS_SECRET_NAME": os.getenv("RDS_SECRET_NAME"),
            "DOCKER_CONTAINER": os.getenv("DOCKER_CONTAINER"),
            "MEMORY_RESOURCE_ID": os.getenv("MEMORY_RESOURCE_ID"),
            "AGENTCORE_RUNTIME_ARN": os.getenv("AGENTCORE_RUNTIME_ARN"),
            "BEDROCK_MODEL_ID": os.getenv("BEDROCK_MODEL_ID"),
            "AWS_ACCESS_KEY_ID": "***" if os.getenv("AWS_ACCESS_KEY_ID") else None,
            "AWS_SECRET_ACCESS_KEY": "***" if os.getenv("AWS_SECRET_ACCESS_KEY") else None,
            "AWS_SESSION_TOKEN": "***" if os.getenv("AWS_SESSION_TOKEN") else None,
            "ENCRYPTION_KEY": "***" if os.getenv("ENCRYPTION_KEY") else None,
            "BEDROCK_CROSS_ACCOUNT_ROLE_ARN": os.getenv("BEDROCK_CROSS_ACCOUNT_ROLE_ARN"),
        },
    }
    print(json.dumps(startup_log, ensure_ascii=False))
    logger.info("ğŸš€ å¯åŠ¨ CostQ AgentCore Runtime")
    logger.info(f"   Host: {args.host}")
    logger.info(f"   Port: {args.port}")
    logger.info(f"   Log Level: {args.log_level}")
    logger.info(f"   Model: {os.getenv('BEDROCK_MODEL_ID')}")
    logger.info("Starting AgentCore Runtime server...")
    print(
        json.dumps(
            {
                "timestamp": datetime.now().isoformat(),
                "level": "INFO",
                "message": "Starting AgentCore Runtime server",
                "host": args.host,
                "port": args.port,
            },
            ensure_ascii=False,
        )
    )
    app.run(host=args.host, port=args.port)
